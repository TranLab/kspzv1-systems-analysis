---
title: "Transcriptomic Baseline Model"
author: "Leetah Senkpeil"
date: "4/5/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r libraries, include=FALSE}
library(openxlsx)
library(xgboost)
library(dplyr)
library(tidyverse)
library(fgsea)
library(glmnet)
library(pROC)
library(aod)
library(caret)
library(limma)
library(caTools)
library(e1071)
library(robustbase)
library(googledrive)

#set plot directory
#plotdir <- "_your_path_here_"

#load source files

temp <- tempfile(fileext = ".rds")

dl <- drive_download(as_id("1togaBlNIxiDj16FyXTC-r7Qw0A9cG2az"), path = temp, overwrite = TRUE)
pfspz_cpm_SM6 <- readRDS(file = dl$local_path)


btms <- drive_download(as_id("1WnjJH07oOLfRfu4dZGj2QVBaDDh70cNm", path = temp, overwrite = TRUE))
lowBTMs <- readRDS(file = btms$local_path)

```

Load Expression Data (logCPM) and calculate pathway activity scores of modules interest
```{r PAS}
#calculate pathway activity scores from log CPM expression values

#myTimepoint <- "Baseline"#not necessary if using timepoint 0 eset only
myDoseGroup <- c("1.8 x 10^6 PfSPZ")

pfspz <- pfspz_cpm_SM6[,pfspz_cpm_SM6$treat %in% myDoseGroup]

#scale data
a <- exprs(pfspz) <- t(scale(t(exprs(pfspz)), center = TRUE, scale = TRUE)) #generating z scores using scale function

rownames(a) <- fData(pfspz)$GeneSymbol

#take out genes in each module from complete expression data frame
annot.btm <- as.data.frame(unlist(names(lowBTMs)))
pas <- c()

for( i in 1:length(lowBTMs)){
  pas[[i]] <- colMedians(matrix(a[intersect(unique(gsub(" ", "",lowBTMs[[i]])),rownames(a)),], ncol = ncol(pfspz)), na.rm = TRUE)#pathway activity score defined as column means/medians depending how COLFUN is defined above 
  names(pas[[i]]) <- colnames(exprs(pfspz))
}

names(pas) <- names(lowBTMs)
pas.df <- do.call("rbind", pas)
pas.df.annot <- merge(annot.btm, pas.df, by.x=1, by.y=0)
pas.df.annot <- na.omit(pas.df.annot)
rownames(pas.df.annot) <- pas.df.annot$`unlist(names(lowBTMs))`
pas.df.annot <- pas.df.annot[,-1]
```


```{r recursive feature elimination}

model_df <- t(pas.df.annot)
outcome <- as.data.frame(pfspz[,which(pfspz$SAMPLEID %in% rownames(model_df))]$mal.atp.3)
rownames(outcome) <- pfspz[,which(pfspz$SAMPLEID %in% rownames(model_df))]$SAMPLEID
colnames(outcome) <- "Outcome3mos"
outcome$Outcome3mos <-  gsub("never_infected", 0, outcome$Outcome3mos)
outcome$Outcome3mos <- gsub("infected", 1, outcome$Outcome3mos)

outcome <- as.factor(unlist(outcome))

temp <- sample_n(as.data.frame(model_df), (nrow(model_df)*(2/3)))
indices <- which(rownames(model_df) %in% rownames(temp))
#saved indices for reproducibility
#indices <- c(2,3,5,7,8,9,10,12,13,14,15,16,17,19,21,22,25,26,27,30,31,32,37,39,42,45,46,47,48,49,50,51,52,53,54,55,56,59,60,61,62,63)

#check size of eset
dim(pfspz)
model_df_tr <- data.frame(model_df[indices,])
outcome_tr <- outcome[indices]

model_df_t <- data.frame(model_df[-indices,])
outcome_t <- outcome[-indices]


featlist <- list()
feat_seeds <- list()


for(iii in c(1:100)){
  temp.seed <- sample.int(10000,1)[[1]]
  set.seed(temp.seed)
  feats <- rfe(model_df_tr, outcome_tr, sizes = 3:5, metric = 'Accuracy', maximize = TRUE, rfeControl = rfeControl())
  featlist[[iii]] <- feats$optVariables[1:5]
  feat_seeds[[i]] <- temp.seed
}

```

```{r find feats}

temp <- unlist(featlist[1])


for(i in 2:100){
  temp2 <- unlist(featlist[i])
  temp <- c(temp,temp2)

}


feat_df <- as.data.frame(unique(temp))
for(i in 1:nrow(feat_df)){ #change to match num of unique features
  feat_df[i,2] <- length(which(temp == feat_df[i,1]))
}

feat_df <- feat_df[order(feat_df[,2], decreasing = T),]#order feats decreasing from most to least chosen


```

with selected features (PAS) use xgboost to predict infection status from transcriptional data
```{r machine learning}


feats <- feat_df[c(1:4),1]#set features to top 4 most common features from 100 rds feat. selection
#feats <- c("spliceosome (M250)","enriched in nuclear pore complex interacting proteins (M247)", "cytokines - recepters cluster (M115)")#manual input of features from saved results of model
pas_df <- pas.df.annot

common <- intersect(rownames(pas_df), feats)

model_df <- as.data.frame(t(pas_df[which(rownames(pas_df) %in% common),]))

pfspz_train <- pfspz_cpm_SM6[,pfspz_cpm_SM6$SAMPLEID %in% rownames(model_df)]
outcome <- as.data.frame(pfspz_train$mal.atp.3)
rownames(outcome) <- pfspz_train$SAMPLEID
colnames(outcome) <- "Outcome3mos"


#check that outcome and model_df are in the same order
model_df$SubjID <- rownames(model_df)
outcome$SubjID <- rownames(outcome)
length(match(rownames(outcome), rownames(model_df)))
model_df <- left_join(model_df, outcome)
model_df$Outcome3mos <- gsub("never_infected", 0, model_df$Outcome3mos)
model_df$Outcome3mos <- gsub("infected", 1, model_df$Outcome3mos)
rownames(model_df) <- model_df$SubjID
model_df <- model_df[,-which(colnames(model_df) == "SubjID")] #Removing SubjID


#create training and test sets
train_df <- model_df[indices,]
train.rows <- rownames(train_df)
test_df <- model_df[-indices,]
test.rows <- rownames(test_df)


train <- matrix(as.numeric(as.character(unlist(train_df))), ncol = ncol(train_df))
colnames(train) <- colnames(train_df)
test <- matrix(as.numeric(unlist(test_df)), ncol = ncol(test_df))
colnames(test) <-  colnames(test_df)
all <- matrix(as.numeric(unlist(model_df)), ncol = ncol(model_df))
colnames(all) <-  colnames(model_df)

#build xgboost datasets
d.train <- xgb.DMatrix(data = train[,-ncol(train)], label = (train[,ncol(train)]))#
d.test <- xgb.DMatrix(data = test[,-ncol(test)], label = (test[,ncol(test)]))#indices need to match training
#d.all <- xgb.DMatrix(data = all[,-ncol(all)], label = (all[,ncol(all)]))

temp.seed <- sample.int(10000,1)[[1]]
#Looping to train best parameters

best_param <- list()
best_seednumber = temp.seed
best_error = Inf

for (iter in 1:1000) {
  print(glue::glue("Curr Iter: ", iter, " Best Error: ", best_error))
  param <- list(objective = 'binary:logistic',
                max_depth = sample(2:15,1),
                eta = runif(1, 0.01, 0.5),
                gamma = sample(c(0, 0.1, 0.2, 0.5, 1, 2, 5, 10), 1),
                subsample = runif(1, 0, 1),
                colsample_bytree = runif(1, 0, 1),
                min_child_weight = sample(0:10, 1),
                lambda = sample(c(0, 0.1, 0.2, 0.5, 1, 2, 5, 10), 1),
                labmda_bias = sample(c(0, 0.1, 0.2, 0.5, 1, 2, 5, 10), 1),
                alpha = sample(c(0, 0.1, 0.2, 0.5, 1, 2, 5, 10), 1)
  )
  cv.nround = 1000
  cv.nfold = 3
  seed.number = sample.int(10000,1)[[1]]
  set.seed(seed.number)
  mdcv <- xgb.cv(data=d.train, verbose = F, params = param, nfold = cv.nfold,
                    nrounds = cv.nround, early_stopping_rounds = 30, maximize = FALSE, metrics = "error")
  min_error = min(mdcv$evaluation_log$test_error_mean)

  if (min_error < best_error) {
    best_error = min_error
    best_nround = which.min(mdcv$evaluation_log$test_error_mean)
    best_seednumber = seed.number
    best_param = param
  }
}

best_param2 <- best_param #set initial model parameters from parameter tuning results

#Manual Parameter Tuning from saved results

best_param2$gamma <- 1.5
best_param2$lambda <- 1.5
best_param2$eta <- 0.422225
best_param2$alpha <- 0.5
best_param2$max_depth <- 11
best_param2$colsample_bytree <- 0.1466715
best_param2$min_child_weight <- 1
best_param2$subsample <- 0.7357056


temp.seed <- sample.int(10000,1)[[1]]#test model with random seeds for reproducibility

# Seeds saved from random temp.seed results with associated errors for reproducibility
#seed1 <- 2851 #28% val error
#seed2 <- 5163 #33% val error
#seed3 <- 447 #28% val error
# seed4 <- 8045 #28% val error
# seed5 <- 6974  #28% val error
# seed6 <- 3337 #28% val error
# seed7 <- 7245 #28% val error
# seed8 <- 6726 #28% val error
#seed9 <- 4472 #28% val error
# seed10 <- 728  #33% val error


set.seed(seed1)#set from saved data
model <- xgb.train(data = d.train, params = best_param2,nrounds = 50000, watchlist = list(val = d.test, train = d.train), early_stopping_rounds = 400, print_every_n = 10, maximize = F, eval_metric = "error")

#using the final model, predict test data classifications
pred <- predict(model, newdata = test[,c(1:(ncol(test)-1))])

#calculate accuracy of model predictions
(sum((pred >= 0.5) == (test[,ncol(test)] == 1)))/length(test[,ncol(test)])

#calculate importance of each feature for the model outcome
imp <- xgb.importance(feature_names = colnames(train[,]), model = model) 
xgb.plot.importance(importance_matrix = imp)

#plot feature importance
plot_imp <- ggplot(data = imp, aes(x = Importance, y = reorder(Feature, Importance))) +
    geom_bar(stat = 'identity',fill = "#d7301f", width = 0.95) +
    theme_classic(base_family = "sans", base_size = 6) +
    theme(axis.ticks = element_line(color = "grey80")) +
    ggtitle("Baseline Transcriptomic Feature Importance") +
    xlab("Importance") + ylab("Features") +
    theme(plot.title=element_text(size =6, hjust = 1, vjust = -1), aspect.ratio = 0.75, axis.text.x.bottom =element_text(size =6)) +
    theme(legend.position = "none")

#export feature importance plot to pdf
cairo_pdf(paste0(plotdir,"Baseline BTMs ML Importance Plot.pdf"),width = 3, height = 2.5)

print(plot_imp)

dev.off()

```

The purpose of this chunk is to make use of xgboostExplainer to understand the impact of each feature on the model -- making logistic regression model more transparent
```{r xgb explain}
library(xgboostExplainer)

#use xgboostexplainer make predicted classifications for each individual in test set
pred.test <- predict(model, newdata = test[,c(1:(ncol(test)-1))])#using test data as newdata -- can use all data according to xgboost documentation
nodes.test <- predict(model, newdata = test[,c(1:(ncol(test)-1))], predleaf = TRUE)

#build tree using model nodes and features
trees <- xgb.model.dt.tree(colnames(d.train), model = model)

#step 1: build explainer
explainer <- buildExplainer(model, d.train, type = "binary")#build explanations for nodes/leafs
pred.breakdown <- explainPredictions(model, explainer, d.test)#predictions explained

#create waterfall plot to demonstrate model feature importance for each feature, one waterfall plot is created for each test subject
for (i in 1:nrow(test)){
water <- showWaterfall(model, explainer, d.test, test[,c(1:(ncol(test)-1))],i,type = "binary")

#export waterfall object to pdf within separate waterfall folder because there are so many files
cairo_pdf(paste0(plotdir,"Waterfall\\Baseline BTM only Waterfall_", seed1, "_", i, "_.pdf"),width = 3, height = 2.5)

print(water)

dev.off()  
}

#print tree constructed from model
trees

```

Write function for custom SHAP summary plot 
```{r xgb shap summary plot}
xgb.ggplot.shap.summary <- function(data, shap_contrib = NULL, features = NULL, top_n = 10, model = NULL,
                                    trees = NULL, target_class = NULL, approxcontrib = FALSE, subsample = NULL) {
  data_list <- xgboost:::xgb.shap.data(
    data = data,
    shap_contrib = shap_contrib,
    features = features,
    top_n = top_n,
    model = model,
    trees = trees,
    target_class = target_class,
    approxcontrib = approxcontrib,
    subsample = subsample,
    max_observations = 10000  # 10,000 samples per feature.
  )
  plot_outcome <- rep(test[,ncol(test)], 3)
  p_data <- xgboost:::prepare.ggplot.shap.data(data_list, normalize = TRUE)
  # Reverse factor levels so that the first level is at the top of the plot
  p_data[, "feature" := factor(feature, rev(levels(feature)))]
  p <- ggplot2::ggplot(p_data, ggplot2::aes(x = feature, y = p_data$shap_value, colour = p_data$feature_value)) +
    ggplot2::geom_jitter(alpha = 1, width = 0.1, height = 0.1, size = 2) +
    ggplot2::scale_colour_distiller(limits = c(-1.5, 1.5), na.value = 0, type = "div", palette = "Spectral", direction = 1) +
    ggplot2::geom_abline(slope = 0, intercept = 0, colour = "darkgrey") +
    ggplot2::coord_flip()

  p
}


```


ROC curves for ML model and SHapley Additive exPlanations dependence plot
```{r ggROC}
library(pROC)

#calculate ROC from all test data
roc1 <- pROC::roc(test[,ncol(test)], pred)
#smoot the curves into one ROC curve
roc1 <- smooth(roc1)

#homebrew function to use ggplot for plotting ROC curve
ggroc <- function(roc, showAUC = TRUE, interval = 0.2, breaks = seq(0, 1, interval)){
  require(pROC)
  if(class(roc) != "roc")
    simpleError("Please provide roc object from pROC package")
  plotx <- rev(roc$specificities)
  ploty <- rev(roc$sensitivities)
  
  ggplot(NULL, aes(x = plotx, y = ploty)) +
    geom_segment(aes(x = 0, y = 1, xend = 1,yend = 0), alpha = 0.5, colour = "red") + 
    geom_step() +
    scale_x_reverse(name = "Specificity",limits = c(1,0), breaks = breaks, expand = c(0.001,0.001)) + 
    scale_y_continuous(name = "Sensitivity", limits = c(0,1), breaks = breaks, expand = c(0.001, 0.001)) +
    theme_bw(base_family = "sans", base_size = 6) +
    theme(axis.ticks = element_line(color = "grey80")) +
    coord_equal() + 
    annotate("text", x = interval*3, y = interval*3, vjust = 0, label = paste("AUC =",sprintf("%.3f",roc$auc)), family = "sans", size = 1) +
    geom_area(fill = "black", alpha = .4) +
    ggtitle("Baseline Transcriptomic Model")+
    theme(plot.title=element_text(size =4))
}

p <- ggroc(roc1)
p + scale_fill_hue(l = 45)

#export ROC curve to pdf file
cairo_pdf(paste0(plotdir,"Baseline BTMs model ROC.pdf"), width = 1.5, height = 1.5)

print(p)

dev.off()
# Set our cutoff threshold
pred.resp <- ifelse(pred >= 0.50, 1, 0)
 
# Create the confusion matrix
cm <- ModelMetrics::confusionMatrix(test[,ncol(test)],pred.resp)

#print confusion matrix, confusion matrix figure is created manually in excel using this data
cm

#create shap plot to visualize feature size and contribution to the model -- alternative SHAP plot included in xgboost package instead of building yourself
#SHAP plot using ggplot for flexible customization
shap <- xgb.ggplot.shap.summary(data = test[,c(1:(ncol(test)-1))], model = model) +
    theme_classic(base_family = "sans", base_size = 8) +
    theme(axis.ticks = element_line(color = "grey80")) +
    ggtitle("Baseline Transcriptomic Global Feature Impact") +
    ylab("SHAP value (impact on model output)") + xlab(NULL) +
    theme(plot.title=element_text(size =8, hjust = 1, vjust = -1), aspect.ratio = .5, axis.text.x =element_text(size =7), axis.text.y =element_text(size =7)) + labs(color = "Feature Value") 

#export SHAP to pdf file  
cairo_pdf(paste0(plotdir,"Baseline SHapley Additive exPlanations BTMs only.pdf"))

print(shap)

dev.off()


```


